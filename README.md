# Unimplicit

Python scripts and R code for the paper '_Pre-trained Language Models' Interpretation of Evaluativity Implicature: Evidence from Gradable Adjectives Usage in Context_' Unimplicit workshop @ NAACL-2022

## Citation
```tex
@inproceedings{cong-2022-pre,
    title = "Pre-trained Language Models' Interpretation of Evaluativity Implicature: Evidence from Gradable Adjectives Usage in Context",
    author = "Cong, Yan",
    booktitle = "Proceedings of the Second Workshop on Understanding Implicit and Underspecified Language",
    month = jul,
    year = "2022",
    address = "Seattle, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.unimplicit-1.1",
    doi = "10.18653/v1/2022.unimplicit-1.1",
    pages = "1--7",
    abstract = "By saying Maria is tall, a human speaker typically implies that Maria is evaluatively tall from the speaker{'}s perspective. However, by using a different construction Maria is taller than Sophie, we cannot infer from Maria and Sophie{'}s relative heights that Maria is evaluatively tall because it is possible for Maria to be taller than Sophie in a context in which they both count as short. Can pre-trained language models (LMs) {``}understand{''} evaulativity (EVAL) inference? To what extent can they discern the EVAL salience of different constructions in a conversation? Will it help LMs{'} implicitness performance if we give LMs a persona such as chill, social, and pragmatically skilled? Our study provides an approach to probing LMs{'} interpretation of EVAL inference by incorporating insights from experimental pragmatics and sociolinguistics. We find that with the appropriate prompt, LMs can succeed in some pragmatic level language understanding tasks. Our study suggests that socio-pragmatics methodology can shed light on the challenging questions in NLP.",
}
```
